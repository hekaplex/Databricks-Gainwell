{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dc8f6b6-9ee9-4e7a-bbcc-46e74d5016e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SQL UDF / ML Example\n",
    "Notes on SQL UDF\n",
    "* When you use a CREATE FUNCTION to make a pure SQL UDF it DOES write to the metadata catalog and persist between cluster restarts\n",
    "* When you use a spark.udf.register to make a Python wrapper for a SQL UDF it DOES NOT write to the metadata catalog and persist between cluster restarts\n",
    "## Catboost to ONNX Conversion Experiment\n",
    "\n",
    "\n",
    "### Outline of experiments\n",
    "1. Persist baseline random dataset for scoring sample of 100,000 rows\n",
    "1. Apply current MLflow model and clock end to end run time\n",
    "1. Generate histogram of output and save output\n",
    "1. Extract native Catboost from MLFlow\n",
    "1. Convert model to ONNX\n",
    "1. Build UDF to predict on native ONNX model \n",
    "1. Apply ONNX model and clock end to end run time\n",
    "1. Persist ONNX results, generate histogram and compare\n",
    "1. Register an ONNX based model in MLFlow\n",
    "1. Import ONNX model and rerun Steps 6-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ef9f6e-be52-4f84-8d19-14bdd7212064",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe2a4d7-2aad-4236-9adb-ebd3bdd9a834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install dbldatagen\n",
    "!pip install catboost\n",
    "!pip install onnx\n",
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb572696-8c83-4dfb-b3ab-0b21795ba294",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Encountered based on DBR runtime being not current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec98196b-8cbd-4fd0-87dd-1e556a80d33a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlflow==1.3\n",
    "!pip install cloudpickle==2.0.0\n",
    "!pip install scikit-learn==0.24.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a78843d0-c8be-46a0-8938-bfcec5317441",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Persist baseline random dataset for scoring sample of 100,000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adfbe9b-d354-47d0-9afe-98322e0f841a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "shuffle_partitions_requested = 8\n",
    "# Control number of Spark Tasks\n",
    "partitions_requested = 32\n",
    "# Total Records to be generated\n",
    "data_rows = 100000\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "# uniqueCustomers = total_records\n",
    "dataspec = (dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested)\n",
    "            .withColumn(\"c1\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c2\", FloatType(), minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c3\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c4\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c5\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c6\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c7\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c8\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c9\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c10\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c11\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c12\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c13\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c14\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c15\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c16\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c17\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c18\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c19\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            .withColumn(\"c20\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            #.withColumn(\"c21\", FloatType(),minValue=0, maxValue=1000, random=True)\n",
    "            )\n",
    "df = dataspec.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "886fc78e-71bd-40d1-bfb5-50eba55305a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save('/dbfs/FileStore/num_cb_op/python_sudf_onnx/numerical_reg_cb_op_1_20col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebf4294b-3b43-44a5-9bb5-dc4a91e1c47a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Apply current MLflow model and clock end to end run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe2815e-74ff-4901-a78b-d74a651f24af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_test = spark.read.format(\"delta\").load('/dbfs/FileStore/num_cb_op/python_sudf_onnx/numerical_reg_cb_op_1_20col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72760741-229f-4158-96e7-cd83f92b46ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import struct, col\n",
    "logged_model = 'runs:/4d02005792134f94b350c615f0637853/model'\n",
    "\n",
    "# Load model\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7ca124-6dd6-4db8-ba89-21e871599729",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import mlflow\n",
    "import catboost\n",
    "@pandas_udf(returnType=DoubleType())\n",
    "def predict_pandas_udf(*features):\n",
    "    \"\"\" Executes the prediction using numpy arrays.\n",
    "         \n",
    "        Parameters\n",
    "        ----------\n",
    "        features : List[pd.Series]\n",
    "            The features for the model, with each feature in it's\n",
    "            owns pandas Series.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "            The predictions.\n",
    "    \"\"\"\n",
    "    # Need a multi-dimensional numpy array for sklearn models.\n",
    "    X = pd.concat(features, axis=1).values\n",
    "    # If model is somewhere in the driver we're good.\n",
    "    y = loaded_model.predict(X)  # <- This is vectorized. Kachow.\n",
    "    return pd.Series(y)\n",
    "\n",
    "catboost_baseline = input_test.withColumn(\n",
    "    \"prediction\",\n",
    "    predict_pandas_udf(col(\"c1\"),\n",
    "col(\"c2\"),\n",
    "col(\"c3\"),\n",
    "col(\"c4\"),\n",
    "col(\"c5\"),\n",
    "col(\"c6\"),\n",
    "col(\"c7\"),\n",
    "col(\"c8\"),\n",
    "col(\"c9\"),\n",
    "col(\"c10\"),\n",
    "col(\"c11\"),\n",
    "col(\"c12\"),\n",
    "col(\"c13\"),\n",
    "col(\"c14\"),\n",
    "col(\"c15\"),\n",
    "col(\"c16\"),\n",
    "col(\"c17\"),\n",
    "col(\"c18\"),\n",
    "col(\"c19\"),\n",
    "col(\"c20\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa051127-2400-4768-8571-318696bd02e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import catboost\n",
    "catboost_baseline.write.format(\"delta\").mode(\"overwrite\").save('/dbfs/FileStore/num_cb_op/python_sudf_onnx/numerical_reg_cb_op_1_20col_baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed23b3a7-e291-40ed-a9e5-ff0679bd2915",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b91a19-d28b-4294-9ed7-b56af558179d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Extract native Catboost from MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45b36bad-f220-4426-890a-dce2e62b6b57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extract model pickle from MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f97eef25-8a8a-46df-992a-490dcae19650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "import pickle\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "tmp_path = client.download_artifacts(run_id='4d02005792134f94b350c615f0637853', path='model/model.pkl')\n",
    "\n",
    "f = open(tmp_path,'rb')\n",
    "\n",
    "model = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "932563b6-3b1e-4a83-8b23-c0786f174476",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Get Catboost model out of SKLearn wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa9a5fb9-e171-4c8c-9ef6-18374390d805",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catmodel = model.best_estimator_\n",
    "type(catmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af006108-9a01-4af5-b1e4-bb618bc51fce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Convert model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a6a3503-6d5b-4a5e-9894-dd10f54fa9b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catmodel.save_model(\n",
    "    \"catboost_ransserachCV.onnx\",\n",
    "    format=\"onnx\",\n",
    "    export_parameters={\n",
    "        'onnx_domain': 'ai.catboost',\n",
    "        'onnx_model_version': 1,\n",
    "        'onnx_doc_string': 'test model for BinaryClassification',\n",
    "        'onnx_graph_name': 'CatBoostModel_for_BinaryClassification'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5067ed58-ec3d-402c-8c27-1173ef5c9206",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Build UDF to predict on native ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa8194c-e6f9-4a4a-a4a3-41d9b79355b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Practice getting ONNX model working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10df7eaf-47f2-49d4-834a-ba56cd094a92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "cat_onnx_sess = rt.InferenceSession(\"catboost_ransserachCV.onnx\") \n",
    "probabilities = cat_onnx_sess.run(['predictions'],\n",
    "                         {'features': dfp.to_numpy()})\n",
    "probabilities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39d339bd-a059-4811-b208-491b0c0fe468",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### UDF and test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7efa9eb-9143-45f0-ba0b-1cd81237472b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    " \n",
    "@pandas_udf(returnType=DoubleType())\n",
    "def predict_pandas_udf_onnx(*features):\n",
    "    \"\"\" Executes the prediction using numpy arrays.\n",
    "         \n",
    "        Parameters\n",
    "        ----------\n",
    "        features : List[pd.Series]\n",
    "            The features for the model, with each feature in it's\n",
    "            owns pandas Series.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "            The predictions.\n",
    "    \"\"\"\n",
    "    # Need a multi-dimensional numpy array for sklearn models.\n",
    "    X = pd.concat(features, axis=1).values\n",
    "\n",
    "\n",
    "    cat_onnx_sess = rt.InferenceSession(\"catboost_ransserachCV.onnx\")      \n",
    "    # If model is somewhere in the driver we're good.\n",
    "    y = cat_onnx_sess.run(['predictions'],\n",
    "                         {'features': features.to_numpy()})[0]  # <- This is vectorized. Kachow.\n",
    "    return pd.Series(y)\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"prediction\",\n",
    "    predict_pandas_udf_onnx(col(\"c1\"),\n",
    "col(\"c2\"),\n",
    "col(\"c3\"),\n",
    "col(\"c4\"),\n",
    "col(\"c5\"),\n",
    "col(\"c6\"),\n",
    "col(\"c7\"),\n",
    "col(\"c8\"),\n",
    "col(\"c9\"),\n",
    "col(\"c10\"),\n",
    "col(\"c11\"),\n",
    "col(\"c12\"),\n",
    "col(\"c13\"),\n",
    "col(\"c14\"),\n",
    "col(\"c15\"),\n",
    "col(\"c16\"),\n",
    "col(\"c17\"),\n",
    "col(\"c18\"),\n",
    "col(\"c19\"),\n",
    "col(\"c20\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26f08ef0-8e16-4756-938d-efc46d4503eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load ONNX model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cc8416c-ae55-457b-af5c-df66169982ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import onnx\n",
    "\n",
    "model_path = \"catboost_ransserachCV.onnx\"\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "416b92f9-a43a-43f5-89b7-f41d5e2981ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create a dummy run to write artifacts and register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d09aa287-cc99-40e5-9598-70751bc8326a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    mlflow.onnx.log_model(onnx_model, artifact_path=\"model\")\n",
    "model_uri = \"runs:/{}/model\".format(run.info.run_id)\n",
    "mv = mlflow.register_model(model_uri, \"CatBoostONNXModel\")    \n",
    "print(\"Name: {}\".format(mv.name))\n",
    "print(\"Version: {}\".format(mv.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2fab3fc-8ce5-4171-aa51-3a378667f737",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 10. Use MLFlow based ONNX model and rerun Steps 6-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b99f84f3-37ac-4f51-ae03-209c2d31469b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Miscellanous code for understanding prior work, debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14c16af9-ce9e-43b1-9687-9ce71cff211f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import catboost\n",
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39b687ea-eec5-4977-9afd-57ca5c7c920d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.tracking\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "run = client.get_run(run_id=\"4d02005792134f94b350c615f0637853\")\n",
    "experiment_id = run.info.experiment_id\n",
    "experiment = client.get_experiment(experiment_id)\n",
    "experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84358ac5-3f8d-4827-adee-fb6c5d0218bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(cat_onnx_sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e968fdc7-e053-429f-85e5-6540e6c9cb81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "def predict_udf(model_uri):\n",
    "    model = mlflow.pyfunc.load_model(model_uri)\n",
    "    @pandas_udf(DoubleType(), PandasUDFType.SCALAR_ITER)\n",
    "    def predict(iterator):\n",
    "        for input_df in iterator:\n",
    "            predictions = model.predict(input_df[['col1', 'col2', 'col3']])\n",
    "            yield predictions\n",
    "    return predict\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1233c713-100a-4881-818b-68c6718ac45b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "def predict_udf_onnx(model_uri):\n",
    "    model = mlflow.pyfunc.load_model(model_uri)\n",
    "    @pandas_udf(DoubleType(), PandasUDFType.SCALAR_ITER)\n",
    "    def predict(iterator):\n",
    "        for input_df in iterator:\n",
    "            predictions = model.predict(input_df[['col1', 'col2', 'col3']])\n",
    "            yield predictions\n",
    "    X = pd.concat(features, axis=1).values\n",
    "\n",
    "\n",
    "    cat_onnx_sess = rt.InferenceSession(\"catboost_ransserachCV.onnx\")      \n",
    "    # If model is somewhere in the driver we're good.\n",
    "    y = cat_onnx_sess.run(['predictions'],\n",
    "                         {'features': features.to_numpy()})[0]  # <- This is vectorized. Kachow.\n",
    "    return pd.Series(y)\n",
    "            \n",
    "    return predict\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cb6390a-c18d-43d3-881b-8e2256791b34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the predict_udf function as a UDF\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "spark.udf.register(\"predict_udf_onnx\", predict_pandas_udf_onnx, DoubleType())\n",
    "\n",
    "# Use the UDF in a SQL query\n",
    "#df = spark.read.parquet(\"data.parquet\")\n",
    "#df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "result = spark.sql(\"SELECT id, predict_udf(models:/CatBoostONNXModel/2)(array(c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20) as prediction FROM input_test_tbl\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c74169a-2ba2-453c-a64b-4eb7377bedfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_test.createOrReplaceTempView(\"input_test_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3516c082-5600-416f-81b7-b400965193af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, ArrayType\n",
    "spark.udf.register(\"predict_pandas_udf_onnx_sql\", predict_pandas_udf_onnx, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4c8b34-c4a9-49f9-9c5f-af61691dfb59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *, predict_udf(c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20) FROM input_test_tbl limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6520df85-5610-4f49-947d-7980ee55372d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fea9101-6a43-493c-928e-9e4058a35535",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri= f'models:/CatBoostONNXModel/2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bcf5024-ab4c-4e57-957e-a643ff1b85eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT   predict_udf($model_uri)(array(c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20) ) as prediction FROM input_test_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da39b35f-819d-4af7-9d8a-6745fc6cb45e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT predict_udf(\"+model_uri+\")(array(c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20) as prediction FROM input_test_tbl\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbbf4d6b-13bb-4e18-a60e-b975331c577e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SQL UDF on Python Predict",
   "notebookOrigID": 1977753182793188,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "db_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c07a7dff8f2193ebeef3a9834e8010caedb79053439254b14be89bc74d6efd66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
