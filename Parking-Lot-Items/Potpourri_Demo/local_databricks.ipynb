{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Client for Databricks DBR 13\n",
    " [DBR <=11.3](https://docs.databricks.com/dev-tools/databricks-connect-legacy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-connect==13.0.*\n",
      "  Using cached databricks_connect-13.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Collecting grpcio-status>=1.48.1\n",
      "  Using cached grpcio_status-1.54.0-py3-none-any.whl (5.1 kB)\n",
      "Collecting databricks-sdk==0.0.7\n",
      "  Using cached databricks_sdk-0.0.7-py3-none-any.whl (211 kB)\n",
      "Collecting pyarrow>=1.0.0\n",
      "  Downloading pyarrow-12.0.0-cp310-cp310-win_amd64.whl (21.5 MB)\n",
      "     ---------------------------------------- 21.5/21.5 MB 9.9 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-metadata 1.7.0 requires protobuf<4,>=3.13, but you have protobuf 4.22.3 which is incompatible.\n",
      "onnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 4.22.3 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googleapis-common-protos>=1.56.4\n",
      "  Using cached googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages (from databricks-connect==13.0.*) (1.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages (from databricks-connect==13.0.*) (1.16.0)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages (from databricks-connect==13.0.*) (1.23.4)\n",
      "Collecting grpcio>=1.48.1\n",
      "  Downloading grpcio-1.54.0-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 10.1 MB/s eta 0:00:00\n",
      "Collecting requests>=2.28.1\n",
      "  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.5/62.5 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.22.3-cp310-abi3-win_amd64.whl (420 kB)\n",
      "     -------------------------------------- 420.6/420.6 kB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.0.5->databricks-connect==13.0.*) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.0.5->databricks-connect==13.0.*) (2.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.28.1->databricks-sdk==0.0.7->databricks-connect==13.0.*) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.28.1->databricks-sdk==0.0.7->databricks-connect==13.0.*) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.28.1->databricks-sdk==0.0.7->databricks-connect==13.0.*) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.28.1->databricks-sdk==0.0.7->databricks-connect==13.0.*) (2.0.12)\n",
      "Installing collected packages: py4j, requests, pyarrow, protobuf, grpcio, googleapis-common-protos, databricks-sdk, grpcio-status, databricks-connect\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.44.0\n",
      "    Uninstalling grpcio-1.44.0:\n",
      "      Successfully uninstalled grpcio-1.44.0\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.55.0\n",
      "    Uninstalling googleapis-common-protos-1.55.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.55.0\n",
      "Successfully installed databricks-connect-13.0.0 databricks-sdk-0.0.7 googleapis-common-protos-1.59.0 grpcio-1.54.0 grpcio-status-1.54.0 protobuf-4.22.3 py4j-0.10.9.7 pyarrow-12.0.0 requests-2.29.0\n"
     ]
    }
   ],
   "source": [
    "# DBR and client version MUST MATCH\n",
    "!pip3 install --upgrade \"databricks-connect==13.0.*\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL adb-4994874874414839.19.azuredatabricks.net\n",
    "# PAT dapi671562d89a0b5a2cc74b406fd5bd5477-3\n",
    "# legacyclusterID 0502-204220-4ffqnr9r\n",
    "# newclusterID 0503-121650-y7r52m4h\n",
    "# OU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Package(s) not found: pyspark\n"
     ]
    }
   ],
   "source": [
    "!pip3 show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks\n",
      "  Downloading databricks-0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: databricks\n",
      "Successfully installed databricks-0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\andre\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# 11.3 !pip3 install databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Spark jar dir: C:\\Users\\andre\\anaconda3\\envs\\Gainwell\\lib\\site-packages\\pyspark\\jars\n",
    "# * Spark home: C:\\Users\\andre\\anaconda3\\envs\\Gainwell\\lib\\site-packages\\pyspark\n",
    "# * Run `pip install -U databricks-connect` to install updates\n",
    "# * Run `pyspark` to launch a Python shell\n",
    "# * Run `spark-shell` to launch a Scala shell\n",
    "# * Run `databricks-connect test` to test connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from databricks.connect import DatabricksSession\n",
    "\n",
    "# spark = DatabricksSession.builder.remote(\n",
    "#   host       = \"https://adb-4994874874414839.19.azuredatabricks.net\",\n",
    "#   token      = \"dapi671562d89a0b5a2cc74b406fd5bd5477-3\",\n",
    "#   cluster_id = \"0502-204220-4ffqnr9r\"\n",
    "# ).getOrCreate()\n",
    "\n",
    "# # Or...\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "spark = DatabricksSession.builder.remote(\n",
    "  \"sc://adb-4994874874414839.19.azuredatabricks.net:443/;token=dapi671562d89a0b5a2cc74b406fd5bd5477-3;x-databricks-cluster-id=0503-121650-y7r52m4h\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0x1ed68f300d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1,100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USER\"]=\"anything\"\n",
    "os.environ[\"SPARK_REMOTE\"] = \"sc://adb-4994874874414839.19.azuredatabricks.net:443/;token=dapi671562d89a0b5a2cc74b406fd5bd5477-3;x-databricks-cluster-id=0503-121650-y7r52m4h\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gainwell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
